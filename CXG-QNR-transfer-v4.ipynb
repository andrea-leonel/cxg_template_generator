{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cbf72a54-364d-40cf-9429-4fa3466b6268",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### 1) Extract the html code from the sat page in CXG Live:\n",
    "> With the S&T pop-up open, right-click 'Inspect' \n",
    "> Do ctrl+f  \n",
    "> Add skipsAndTriggersHistoryContainer in the search bar \n",
    "> Right-click the highlighted line\n",
    "> Copy > copy element > paste it in a notepad\n",
    "> Save it as html_{protoid}.html - replace protoid with the survey's protoid\n",
    "> On Jupyter's folder (the same folder where this script is saved), upload the html file. \n",
    "\n",
    "### 2) Extract DataList on CXG Connect:\n",
    "> Go Questionnaire Builder on the left handside menu\n",
    "> Serch your survey name and select it\n",
    "> Click Export S&T\n",
    "> In the excel file downloaded, there will be a tab called 'DataList', right-click it > Move or Copy\n",
    "> Under To book: click (new book) and save it as Datalist_{protoid} - replace protoid with the survey's protoid\n",
    "> On Jupyter's folder (the same folder where this script is saved), upload the html file. \n",
    "\n",
    "### 3) Add the protoids you'd like to process to the script.\n",
    "> Below there is a variable called protoid, add the protoids number there, always in this format ['protoid1','protoid2','protoid3',...,'protoid16']\n",
    "> with the cell below selected, press shift+Enter.\n",
    "> The templates will be saved in the same folder as the script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "589f0af6-0234-4ad9-9b7e-e60b92176d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved template files for: 29401\n",
      "Processed and saved template files for: 29352\n",
      "Processed and saved template files for: 29463\n",
      "Processed and saved template files for: 29249\n",
      "Processed and saved template files for: 29244\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from io import StringIO\n",
    "import csv\n",
    "import re\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import PatternFill, Alignment, Font\n",
    "import traceback\n",
    "\n",
    "\n",
    "protoids = ['29401','29352','29463','29249','29244']\n",
    "\n",
    "def generate_sat_template(protoid):\n",
    "\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "    try:\n",
    "        with open(f\"html_{protoid}.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            trigger_history_container = file.read()\n",
    "    except FileNotFoundError:\n",
    "        print(\"The file does not exist!\")\n",
    "\n",
    "    # Parse the div content with BeautifulSoup\n",
    "    soup = BeautifulSoup(trigger_history_container, 'html5lib')\n",
    "\n",
    "    # Find the conditions div \"box-main\"\n",
    "    box_main = soup.find_all('div', {'class': 'box-main'}) #box_main is a list of div elements, each containing a condition\n",
    "    conditions = pd.DataFrame()\n",
    "    actions = pd.DataFrame()\n",
    "    group_n = []\n",
    "    condition_n = []\n",
    "\n",
    "    sat_n = 0\n",
    "    for element in box_main: # under each sat\n",
    "    \n",
    "        if \"(Suspended)\" in str(element): # Skipping sats with (Suspended) questions\n",
    "            continue\n",
    "\n",
    "        sat_n += 1 # defines trigger number\n",
    "    \n",
    "        # Extracting condition table\n",
    "        condition_table = element.find('table', id=lambda x: x and x.startswith('conditionDescriptionTable'))\n",
    "        block_exclude = element.find_all('div', {'class': 'historySectionLevelsDiv'}) # Finds section names\n",
    "        block_text = []\n",
    "        tr = condition_table.find_all('tr')\n",
    "\n",
    "        for row in tr: # each row is a line of the condition\n",
    "\n",
    "            # take the condition\n",
    "            condition_text = row.get_text(strip=True)\n",
    "\n",
    "            exclude_texts = [tag.get_text(strip=True) for tag in block_exclude if tag]\n",
    "\n",
    "            for text_exclude in exclude_texts:  # Iterate through the extracted texts\n",
    "                if text_exclude in condition_text:  # Check if the text exists in condition_text\n",
    "                    condition_text = condition_text.replace(text_exclude, '')  # Perform the replacement\n",
    "            \n",
    "            #take the condition and group number, if any\n",
    "            input_tag = row.find('input', {'name': 'editremove'})\n",
    "            #print(input_tag)\n",
    "            if input_tag:\n",
    "                value_attr = input_tag.get('value')\n",
    "                if value_attr:  # Ensure value_attr is not None\n",
    "\n",
    "                    if '\"group\":' in value_attr:\n",
    "                        group_start = value_attr.find('\"group\":') + len('\"group\":')\n",
    "                        group_end = value_attr.find(',', group_start)\n",
    "                        group_n = value_attr[group_start:group_end].strip()\n",
    "                    else:\n",
    "                        group_n = None\n",
    "        \n",
    "                    if '\"condition\":' in value_attr:\n",
    "                        condition_start = value_attr.find('\"condition\":') + len('\"condition\":')\n",
    "                        condition_end = value_attr.find(',', condition_start) if ',' in value_attr[condition_start:] else value_attr.find('}', condition_start)\n",
    "                        condition_n = value_attr[condition_start:condition_end].strip()\n",
    "                    else:\n",
    "                        condition_n = None\n",
    "\n",
    "            block_text.append({'sat_n': sat_n, 'group_n': group_n, 'condition_n': condition_n, 'condition_text': condition_text})\n",
    "\n",
    "        df_condition = pd.DataFrame(block_text, columns=block_text[0])\n",
    "        conditions = pd.concat([conditions,df_condition],ignore_index=True)\n",
    "    \n",
    "        # Extracting action table\n",
    "        action_table = element.find('table', id=lambda x: x and x.startswith('historyActionTable'))\n",
    "        if action_table:\n",
    "            # Convert the second table into a dataframe\n",
    "            rows = action_table.find_all('tr')\n",
    "            table_data = []\n",
    "            for row in rows:\n",
    "                cols = row.find_all(['th', 'td'])\n",
    "                cols_text = [col.get_text(strip=True) for col in cols]\n",
    "                table_data.append(cols_text)\n",
    "\n",
    "        # Storing the action table in a DataFrame and adding the condition in a column\n",
    "        df_action = pd.DataFrame(table_data[1:], columns=table_data[0])  # [1:] skips the header row, table_data[0] is the header\n",
    "        df_action['sat_n'] = sat_n\n",
    "        actions = pd.concat([actions,df_action],ignore_index=True)\n",
    "\n",
    "    sat = pd.merge(conditions, actions, on='sat_n', how='left') # Appending all the dataframes for each box_main (sat)\n",
    "    sat['group_n'] = sat['group_n'].astype(int) + 1\n",
    "    sat['condition_n'] = sat['condition_n'].astype(int) + 1\n",
    "    sat['group_n'] = sat['group_n'].apply(lambda x: f\"Group {int(x)}\")\n",
    "    sat['condition_n'] = sat['condition_n'].apply(lambda x: f\"Condition {int(x)}\")\n",
    "    \n",
    "    # Renaming columns\n",
    "    new_column_names = {\n",
    "        'sat_n': 'sat_n',\n",
    "        'condition_text': 'cond_question',\n",
    "        'Action': 'act_action_question',\n",
    "        'Question': 'act_parameter',\n",
    "        'Answers': 'act_action_answer',\n",
    "        'Comment': 'act_action_comment'\n",
    "    }\n",
    "    sat = sat.rename(columns=new_column_names)\n",
    "    sat = sat[['sat_n','group_n','condition_n','cond_question', 'act_parameter','act_action_answer', 'act_action_comment','act_action_question']]\n",
    "    sat = sat.sort_values(by=['sat_n','group_n','condition_n'], ascending=[True, True, True])\n",
    "\n",
    "    # Breaking the tables into action and condition to be processed separately\n",
    "    conditions = sat[['sat_n','group_n','condition_n','cond_question']]\n",
    "    actions = sat[['sat_n','act_parameter','act_action_answer', 'act_action_comment','act_action_question']]\n",
    "\n",
    "    # Condition processing\n",
    "\n",
    "    # Breaking down cond_question\n",
    "\n",
    "    conditions = conditions.copy()\n",
    "\n",
    "    # Breaking down cond_question\n",
    "    conditions.loc[:,'cond_logical_operator'] = conditions['cond_question'].apply(lambda x: re.search(r'(AND|OR)', str(x)).group(0) if re.search(r'(AND|OR)', str(x)) else None) # picks up the words AND or OR, if any\n",
    "    conditions.loc[:,'cond_question'] = conditions.apply(lambda row: row['cond_question'].replace(row['cond_logical_operator'], '') if row['cond_logical_operator'] else row['cond_question'],axis=1) # removes logical operator from main string, if any\n",
    "    conditions.loc[:,'cond_operator'] = conditions['cond_question'].apply(lambda x: re.search(r'\\b[A-Z][A-Z\\s]*\\b', str(x)).group(0) if re.search(r'\\b[A-Z][A-Z\\s]*\\b', str(x)) else None) # picks up capitalised strings (IS IN, IS NOT IN...)\n",
    "    conditions.loc[:, 'cond_parameter'] = conditions.apply(lambda row: (row['cond_question'].split(' [')[0] if isinstance(row['cond_question'], str) and ' [' in row['cond_question'] else (row['cond_question'].split(row['cond_operator'])[0].strip() if row['cond_operator'] and row['cond_operator'] in row['cond_question'] else None)),axis=1)\n",
    "    conditions['cond_parameter_value'] = conditions['cond_question'].apply(lambda x: x.split(\" (\")[1].split(\")\")[0] if \" (\" in x and \")\" in x else None) # takes string between ()\n",
    "\n",
    "    # Remove Question ID from cond_question\n",
    "    conditions.loc[:, 'cond_parameter'] = conditions['cond_parameter'].apply(lambda x: x.split(' [')[0] if isinstance(x, str) and ' [' in x else x)\n",
    "\n",
    "    # Set Parameter_type is always Answer for Condition\n",
    "    conditions.loc[:,'cond_parameter_type'] = 'Answer'\n",
    "\n",
    "    # Creating extra columns Type and Name\n",
    "    conditions['cond_type'] = 'Condition'     \n",
    "\n",
    "    # Conditionally update rows containing \"Group\"\n",
    "    conditions.loc[conditions['cond_question'].str.contains('Group', case=False, na=False),['condition_n','cond_operator', 'cond_parameter', 'parameter_value']] = [None, None, None, None]\n",
    "    conditions.loc[conditions['cond_question'].str.contains('Group', case=False, na=False), 'cond_parameter_type'] = \"Group\"\n",
    "    conditions.loc[~conditions['cond_question'].str.contains('Group', case=False, na=False), 'cond_parameter_type'] = \"Answer\"\n",
    "\n",
    "    # Adding empty columns for unioning\n",
    "    conditions.loc[:,'cond_action'] = None\n",
    "    conditions.loc[:,'cond_action_option'] = None\n",
    "\n",
    "    conditions = conditions[['sat_n','group_n','condition_n','cond_type','cond_parameter_type','cond_parameter','cond_operator','cond_parameter_value','cond_action','cond_action_option','cond_logical_operator']]\n",
    "\n",
    "    # Break down condition answers and placing them in different columns\n",
    "    answer_split = conditions['cond_parameter_value'].str.split(';', expand=True) # Split by ;\n",
    "    answer_split.columns = [f'cond_parameter_value_{i+1}' for i in range(answer_split.shape[1])] # assigning each answer to its own column with an identifier\n",
    "    conditions = pd.concat([conditions,answer_split], axis=1) # Appending new columns\n",
    "\n",
    "    # Turning the answers into rows\n",
    "    conditions_unpivot = pd.melt(conditions, \n",
    "                       id_vars=['sat_n','group_n','condition_n','cond_type','cond_parameter_type','cond_parameter','cond_operator','cond_action','cond_action_option','cond_logical_operator'], \n",
    "                       value_vars=answer_split.columns,\n",
    "                       var_name='answer', \n",
    "                       value_name='parameter_value')\n",
    "\n",
    "    # Tidying the table and adding empty columns to align with the action table (for the union)\n",
    "    conditions_final = conditions_unpivot.rename(columns={'parameter_value': 'cond_parameter_value'})\n",
    "    conditions_final = conditions_final[['sat_n','group_n','condition_n','cond_type','cond_parameter_type','cond_parameter','cond_operator','cond_parameter_value','cond_action','cond_action_option','cond_logical_operator']]\n",
    "    conditions_final = conditions_final.drop_duplicates()\n",
    "    conditions_final = conditions_final[conditions_final['cond_parameter_value'].notna()]\n",
    "    conditions_final = conditions_final.sort_values(by=['sat_n','group_n','condition_n'], ascending=[True, True, True])\n",
    "\n",
    "    # Action processing - part 1\n",
    "\n",
    "    # Attachement\n",
    "    actions = actions.copy()\n",
    "\n",
    "    # Remove Question ID from act_parameter\n",
    "    actions.loc[:,'act_parameter'] = actions['act_parameter'].apply(lambda x: x.split(' [')[0])\n",
    "\n",
    "    # Setting up Types for Answer and Comment, normalising Comment action\n",
    "    actions.loc[:, 'Parameter Type_1'] = actions['act_action_answer'].apply(lambda x: 'Answer' if x != \"\" else None)\n",
    "    actions.loc[:, 'Parameter Type_2'] = actions['act_action_comment'].apply(lambda x: 'Comment' if x != \"\" else None)\n",
    "    actions.loc[:,'act_action_comment'] = actions['act_action_comment'].apply(lambda x: 'Change' if x == 'Set Comment(comment: )' else None)\n",
    "\n",
    "    # Break down act_action_answer (separating the action-answer combinations)\n",
    "    action_split = actions['act_action_answer'].str.split(')', expand=True)\n",
    "    action_split.columns = [f'act_action_answer_{i+1}' for i in range(action_split.shape[1])] # assigning each action-answer to its own column with an identifier\n",
    "    actions = pd.concat([actions, action_split], axis=1)\n",
    "\n",
    "    # Separate action-answer into respective columns with identifier\n",
    "    action_split2 = pd.DataFrame()\n",
    "    for i in range(action_split.shape[1]): #action_split.shape[1] is the number of action-answer combinations\n",
    "        column_name = f'act_action_answer_{i+1}'\n",
    "        action_split2[[f'act_action_{i+1}', f'act_parameter_value_{i+1}']] = actions[column_name].apply(\n",
    "            lambda x: pd.Series(str(x).split('(', 1)) if isinstance(x, str) and '(' in str(x) else pd.Series([None, None]))\n",
    "\n",
    "    actions = actions[['sat_n','act_parameter', 'act_action_comment','act_action_question']]\n",
    "    actions = pd.concat([actions, action_split2], axis=1)\n",
    "\n",
    "    # Action processing - part 2\n",
    "\n",
    "    # Unpivotting the columns to have actions and answers as rows pertaining to the question\n",
    "\n",
    "    # Unpivot parameter_type=comment actions\n",
    "    unpivoted_1 = pd.melt(actions, id_vars=['sat_n', 'act_parameter'], value_vars=['act_action_comment'], var_name='act_parameter_type', value_name='act_action')\n",
    "    unpivoted_1 = unpivoted_1[unpivoted_1['act_action'].notna()]\n",
    "    unpivoted_1.loc[:,\"act_parameter_type\"] = \"Comment\"\n",
    "\n",
    "    # Unpivot parameter_type=question actions\n",
    "    unpivoted_2 = pd.melt(actions, id_vars=['sat_n', 'act_parameter'], value_vars=['act_action_question'], var_name='act_parameter_type', value_name='act_action')\n",
    "    unpivoted_2 = unpivoted_2[unpivoted_2['act_action'].notna()]\n",
    "    unpivoted_2.loc[:,\"act_parameter_type\"] = \"Question\"\n",
    "\n",
    "    # Unioning them and setting up empty column to align with the parameter_type=answer table below\n",
    "    question_answer = pd.concat([unpivoted_1,unpivoted_2], ignore_index=True)\n",
    "    question_answer.loc[:,\"act_parameter_value\"] = None\n",
    "    question_answer = question_answer[['sat_n','act_parameter_type','act_parameter', \"act_parameter_value\",'act_action']]\n",
    "\n",
    "    question_answer = question_answer.drop_duplicates()\n",
    "\n",
    "    # Actions processing - part 3\n",
    "\n",
    "    # Manipulating the parameter_type=answer actions and values to be in the template shape\n",
    "\n",
    "    # Unpivotting the actions, defining identifier column for the action-answer combination (action_n), setting up id for left join\n",
    "    unpivoted_dfs = []\n",
    "    action_columns = [col for col in actions.columns if col.startswith('act_action_') and col.split('_')[-1].isdigit()]\n",
    "    for act_action in action_columns:\n",
    "        unpivoted_n = pd.melt(actions, id_vars=['sat_n', 'act_parameter'], value_vars=[act_action], var_name='act_parameter_type', value_name='value')\n",
    "        unpivoted_dfs.append(unpivoted_n)\n",
    "\n",
    "    action_unpivoted= pd.concat(unpivoted_dfs, ignore_index=True)\n",
    "    action_unpivoted = action_unpivoted[action_unpivoted['value'].notna()]\n",
    "    action_unpivoted.loc[:, 'action_n'] = action_unpivoted['act_parameter_type'].apply(lambda x: x.split('_')[-1] if x.split('_')[-1].isdigit() else None)\n",
    "    action_unpivoted.loc[:, 'act_parameter_type'] = action_unpivoted['act_parameter_type'].apply(lambda x: \"act_action\" if \"action\" in x else None)\n",
    "    action_unpivoted.loc[:, 'id'] = action_unpivoted['sat_n'].astype(str) + action_unpivoted['act_parameter'].astype(str) + action_unpivoted['action_n'].astype(str)\n",
    "\n",
    "    # Unpivotting the answers, defining identifier column for the action-answer combination (action_n), setting up id for left join\n",
    "    unpivoted_dfs = []\n",
    "    param_columns = [col for col in actions.columns if col.startswith('act_parameter_value_')]\n",
    "    for act_param in param_columns:\n",
    "        unpivoted_n = pd.melt(actions, id_vars=['sat_n', 'act_parameter'], value_vars=[act_param], var_name='act_parameter_type', value_name='value')\n",
    "        unpivoted_dfs.append(unpivoted_n)\n",
    "\n",
    "    param_unpivoted= pd.concat(unpivoted_dfs, ignore_index=True)\n",
    "    param_unpivoted = param_unpivoted[param_unpivoted['value'].notna()]\n",
    "    param_unpivoted.loc[:, 'action_n'] = param_unpivoted['act_parameter_type'].apply(lambda x: x.split('_')[-1] if x.split('_')[-1].isdigit() else None)\n",
    "    param_unpivoted.loc[:, 'act_parameter_type'] = param_unpivoted['act_parameter_type'].apply(lambda x: \"act_parameter_value\" if \"parameter_value\" in x else None)\n",
    "    param_unpivoted.loc[:, 'id'] = param_unpivoted['sat_n'].astype(str) + param_unpivoted['act_parameter'].astype(str) + param_unpivoted['action_n'].astype(str)\n",
    "\n",
    "    # Left join attaching the answers to the actions (there'll always be an action, but not always an answer)\n",
    "    actions_param = pd.merge(action_unpivoted, param_unpivoted, how='left', on='id',suffixes=('_action', '_parameter'))\n",
    "\n",
    "    # Tidying up the table, setting up type\n",
    "    actions_param = actions_param.sort_values(by=['sat_n_action','act_parameter_action','action_n_action'], ascending=[True, True, True])\n",
    "    actions_param.loc[:,\"act_parameter_type\"] = \"Answer\"\n",
    "    actions_param = actions_param[['sat_n_action','act_parameter_type','act_parameter_action','value_parameter','value_action']]\n",
    "\n",
    "    actions_param = actions_param.rename(columns={\n",
    "        'sat_n_action': 'sat_n',\n",
    "        'act_parameter_type': 'act_parameter_type',\n",
    "        'act_parameter_action': 'act_parameter',\n",
    "        'value_parameter': 'act_parameter_value',\n",
    "        'value_action': 'act_action'\n",
    "    })\n",
    "\n",
    "    actions_param = actions_param.drop_duplicates()\n",
    "\n",
    "    # Actions processing - part 4\n",
    "\n",
    "    # Unioning the parameter_types together and adding empty columns to align with Conditions table for unioning\n",
    "    actions_final = pd.concat([question_answer, actions_param], ignore_index=True)\n",
    "    actions_final.loc[:,\"act_type\"] = \"Action\"\n",
    "    actions_final.loc[:,\"act_group_n\"] = None\n",
    "    actions_final.loc[:,\"act_condition_n\"] = None\n",
    "    actions_final.loc[:,\"act_operator\"] = None\n",
    "    actions_final.loc[:,\"act_action_option\"] = None\n",
    "    actions_final.loc[:,\"act_logical_operator\"] = None\n",
    "\n",
    "    actions_final = actions_final.sort_values(by=['sat_n','act_parameter_type','act_parameter','act_action'], ascending=[True, True, True,True])\n",
    "    actions_final = actions_final[['sat_n','act_group_n','act_condition_n','act_type','act_parameter_type','act_parameter','act_operator','act_parameter_value','act_action','act_action_option','act_logical_operator']]\n",
    "\n",
    "    actions_final = actions_final.drop_duplicates()\n",
    "\n",
    "    # Merging conditions and actions\n",
    "\n",
    "    # renaming conditions columns\n",
    "    conditions_final = conditions_final.rename(columns={\n",
    "        'sat_n': 'sat_n',\n",
    "        'cond_type': 'type',\n",
    "        'cond_parameter_type': 'parameter_type',\n",
    "        'cond_parameter': 'parameter',\n",
    "        'cond_operator': 'operator',\n",
    "        'cond_parameter_value': 'parameter_value',\n",
    "        'cond_action': 'action',\n",
    "        'cond_action_option': 'action_option',\n",
    "        'cond_logical_operator': 'logical_operator',\n",
    "    })\n",
    "\n",
    "    # renaming action columns\n",
    "    actions_final = actions_final.rename(columns={\n",
    "        'sat_n': 'sat_n',\n",
    "        'act_type': 'type',\n",
    "        'act_parameter_type': 'parameter_type',\n",
    "        'act_parameter': 'parameter',\n",
    "        'act_operator': 'operator',\n",
    "        'act_parameter_value': 'parameter_value',\n",
    "        'act_action' : 'action',\n",
    "        'act_action_option': 'action_option',\n",
    "        'act_logical_operator': 'logical_operator',\n",
    "    })\n",
    "\n",
    "    sat_combined = pd.concat([conditions_final, actions_final], ignore_index=True)\n",
    "    sat_combined = sat_combined.sort_values(by=['sat_n','type','parameter'], ascending=[True, False, True])\n",
    "\n",
    "    # Equivalencies of operators and actions\n",
    "\n",
    "\n",
    "    # Operator equivalence\n",
    "    sat_combined['operator'] = sat_combined['operator'].apply(lambda x: 'in' if x == 'IS IN' else '!in' if x == 'IS NOT IN' else None)\n",
    "\n",
    "    # Action equivalence\n",
    "\n",
    "    #add attachment\n",
    "\n",
    "    action_mapping = {\n",
    "        'show': 'Show',\n",
    "        'hide': 'Hide',\n",
    "        'Change': 'Change',\n",
    "        'Set Answer': 'Set',\n",
    "        'Clear Answer': 'Clear',\n",
    "        'Disable Answer': 'Disable',\n",
    "        'Enable Answer': 'Enable',\n",
    "        'Show Answer': 'Show',\n",
    "        'Hide Answer': 'Hide',\n",
    "    }\n",
    "    sat_combined['action'] = sat_combined['action'].apply(lambda x: action_mapping.get(x, None))\n",
    "\n",
    "    # Formatting the final table to the template shape\n",
    "\n",
    "    # Function to add \"Rule\" and \"Group\" rows\n",
    "    def add_null_rows(group):\n",
    "        rule_row = {'sat_n': group['sat_n'].iloc[0], 'type': 'Rule'}\n",
    "        group_row = {'sat_n': group['sat_n'].iloc[0], 'type': 'Group'}\n",
    "        return pd.concat([pd.DataFrame([rule_row, group_row]), group])\n",
    "\n",
    "    # Apply the function and grouping by sat_n\n",
    "    sat_final = sat_combined.groupby('sat_n')[sat_combined.columns.tolist()].apply(add_null_rows).reset_index(drop=True)\n",
    "\n",
    "    # Add Name column after type\n",
    "    sat_final.loc[:,'name'] = sat_final.apply(lambda row: sat_final.loc[row.name + 1, 'group_n'] if row['type'] == 'Group' and row.name + 1 < len(sat_final) else (row['condition_n'] if row['type'] == 'Condition' else None), axis=1)\n",
    "\n",
    "    sat_final = sat_final[['sat_n','type','name','parameter_type','parameter','operator','parameter_value','action','action_option','logical_operator']]    \n",
    "\n",
    "    # Looking up question codes and replacing\n",
    "\n",
    "    # Load your CSV file into a DataFrame\n",
    "    datalist = pd.read_excel(f'datalist_{protoid}.xlsx', sheet_name='DataLists')\n",
    "\n",
    "    # Create a new column to store the matching values\n",
    "    sat_final['parameter'] = sat_final['parameter'].apply(lambda param: next((row for row in datalist.iloc[:, 0] if str(param) in str(row)),param))\n",
    "\n",
    "    # Creating excel file\n",
    "\n",
    "    # Aligning column names\n",
    "    sat_final = sat_final[['type','name','parameter_type','parameter','operator','parameter_value','action','action_option','logical_operator']]\n",
    "    sat_final = sat_final.rename(columns={\n",
    "        'name': 'Name',\n",
    "        'parameter_type': 'Parameter Type',\n",
    "        'parameter': 'Parameter',\n",
    "        'operator': 'Statement Operator',\n",
    "        'parameter_value': 'Parameter Value',\n",
    "        'action': 'Action',\n",
    "        'action_option': 'Action Option',\n",
    "        'logical_operator': 'Logical Operator'\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # Create a new workbook and select the active sheet\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "\n",
    "    # Write the dataframe into the sheet, including headers\n",
    "    for col_idx, column in enumerate(sat_final.columns, start=1):\n",
    "        cell = ws.cell(row=1, column=col_idx, value=column) \n",
    "        cell.font = Font(bold=True)  # Make headers bold\n",
    "        cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")  # Center the text\n",
    "\n",
    "    for row_idx, row in sat_final.iterrows():\n",
    "        for col_idx, value in enumerate(row, start=1):\n",
    "            cell = ws.cell(row=row_idx + 2, column=col_idx, value=value)  # Data rows\n",
    "            cell.alignment = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "\n",
    "    # Define the fill colors\n",
    "    colors = {\n",
    "        'Rule': 'D88A55',\n",
    "        'Group': '00B09B',\n",
    "        'Condition': '9DD3C9',\n",
    "        'Action': 'D9D9D9'\n",
    "    }\n",
    "\n",
    "    # Apply conditional formatting row by row\n",
    "    for row_idx, row in sat_final.iterrows():\n",
    "        fill_color = colors.get(row['type'], None)\n",
    "        if fill_color:\n",
    "            for col_idx in range(1, len(sat_final.columns) + 1):  # Format all cells in the row\n",
    "                ws.cell(row=row_idx + 2, column=col_idx).fill = PatternFill(\n",
    "                    start_color=fill_color,\n",
    "                    end_color=fill_color,\n",
    "                    fill_type=\"solid\"\n",
    "                )\n",
    "\n",
    "    for column_cells in ws.columns:\n",
    "        ws.column_dimensions[column_cells[0].column_letter].width = 20\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(f'template_{protoid}.xlsx')\n",
    "\n",
    "for protoid in protoids:\n",
    "    try:\n",
    "        generate_sat_template(protoid)\n",
    "        print(f\"Processed and saved template files for: {protoid}\")\n",
    "    except Exception as e:\n",
    "        tb = traceback.extract_tb(e.__traceback__)\n",
    "        line_number = tb[-1].lineno \n",
    "        print(f\"Error processing {protoid}: {e}\")\n",
    "        print(f\"Error occurred on line number: {line_number}\")\n",
    "        print(f\"Failed operation: {tb[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "662d61ed-e1fe-4292-a340-ffe9adfdeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d136203d-f1f3-473a-bef0-46c1da70852a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
